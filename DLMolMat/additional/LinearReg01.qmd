---
title: Regresi linear sederhana
lang: id
language:
  id:
    crossref-eq-prefix: Persamaan
    section-title-abstract: Abstrak
jupyter:
  jupytext:
    text_representation:
      extension: .qmd
      format_name: quarto
      format_version: '1.0'
      jupytext_version: 1.15.2
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

```{python}
import numpy as np
```

Materi ini diadaptasi dari
- Simon Rogers dan Mark Girolami. First Course in Machine Learning (2nd Ed). Bab 1

# Regresi linear

Kita akan mulai pembahasan kita dengan meninjau permasalahan
regresi linear.

```{python}
import matplotlib.pyplot as plt
import matplotlib
```

Bagian ini opsional (untuk membuat format default dari Matplotlib berupa svg)

```{python}
import matplotlib_inline
matplotlib_inline.backend_inline.set_matplotlib_formats("svg")
```

Bagian ini opsional:

```{python}
matplotlib.style.use("dark_background")
matplotlib.rcParams.update({
    "axes.grid" : True,
    "grid.color": "gray",
    "grid.linestyle": "--"
})
```

Misalkan kita telah mengetahui bahwa data kita berasal dari fungsi
berikut:
$$
f(x) = 2x + 1
$$
Fungsi ini akan didefinisikan dengan menggunakan kode Python berikut:

```{python}
def f_true(x):
    return 2*x + 1
```

```{python}
f_true(1.1)
```

$$
y = f(x) + \epsilon
$$
$\epsilon \sim \mathcal{N}(0,\sigma)$

Pada kenyataannya, fungsi ini akan dipengaruhi oleh noise. Kita akan mensimulasikan noise
ini dengan bilangan acak yang terdistribusi normal dengan rata-rata 0 dan deviasi
standard $\sigma$ (atau amplitudo `A_noise`). Anda dapat mencoba mengubah-ubah nilai `A_noise` ini.

Selain itu, kita juga memilih sampel titik-titik $x$ berada pada interval $[-5,5]$ dengan
jarak pisah yang seragam. Kita dapat menggunakan fungsi `np.linspace` untuk keperluan ini.

```{python}
np.random.seed(1234) # nilai awal random generator
Npoints = 10
x = np.linspace(-5.0, 5.0, Npoints)
A_noise = 2.4 # standard deviation or amplitude of noise
y = f_true(x) + A_noise*np.random.randn(Npoints)
plt.scatter(x, y);
plt.grid(True)
```

Berikut adalah "data" yang kita miliki:

```{python}
import pandas as pd
pd.DataFrame( list(zip(x, y)) )
```

Misalkan sekarang kita ingin mencocokkan fungsi linear $\hat{f}(x)$ untuk data ini:
$$
\hat{f}(x) = w_0 + w_1 x
$$ {#eq-model-linear-01}
dengan $w_0$ dan $w_1$ adalah parameter model.

**Bagaimana cara memilih parameter model yang baik?**

**Bagaimana membedakan antara model yang baik dan yang tidak atau kurang baik?**

Sekarang permasalahan yang dihadapi adalah bagaimana memilih parameter
model yang terbaik. Sebelum itu, kita perlu mendefinisikan suatu kuantitas
atau metrik atau ukuran yang dapat dijadikan indikator seberapa baik model
untuk dataset yang diberikan.
Salah satu metrik yang dapat digunakan adalah beda kuadrat antara prediksi
model dan data target. Untuk data ke-$n$ dapat dituliskan sebagai berikut.
$$
\mathcal{L}_n \equiv \left( y_n - f(x_n; w_0, w_1) \right)^2
$$

Dengan merata-ratakan kontribusi dari seluruh data, dapat diperoleh
$$
\mathcal{L} = \frac{1}{N} \sum_{n=1}^{N} \mathcal{L}_n =
\frac{1}{N} \sum_{n=1}^{N} \left( y_n - f(x_n; w_0, w_1) \right)^2
$$ {#eq-loss-function-01}
Kuantitas ini dikenal sebagai fungsi rugi atau *loss function*.

Perhatikan bahwa fungsi rugi selalu bernilai positif atau sama dengan nol.
Jika kuantitas ini bernilai nol maka model yang digunakan dapat memberikan
prediksi yang tepat sama dengan data target. Secara praktis hal ini jarang
ditemui. Meskipun demikian, kita ingin fungsi rugi bernilai sekecil mungkin
sehingga prediksi model tidak jauh berbeda dengan target.
Untuk menemukan nilai fungsi rugi yang sekecil mungkin kita dapat melakukan
proses yang dikenal sebagai minimisasi atau optimasi fungsi.

Secara umu, kita dapat mencari parameter $(w_{0},w_{1})$ dengan menggunakan
prosedur minimisasi
yang secara matematis dapat dituliskan sebagai berikut.
$$
\arg\min_{w_{0},w_{1}} \frac{1}{N} \sum_{n=1}^{N} \mathcal{L}_{n}
$$

Untuk kasus khusus fungsi rugi pada Persamaan (@eq-loss-function-01), kita dapat
menemukannya secara analitik dengan menggunakan prinsip kalkulus: cari turunan pertama
dari  $\mathcal{L}$ terhadap $w_0$ dan $w_1$, kemudian mencari solusi dari:
$$
\begin{align*}
\frac{\partial\mathcal{L}}{\partial w_0} & = 0 \\
\frac{\partial\mathcal{L}}{\partial w_1} & = 0
\end{align*}
$$
Untuk fungsi rugi yang lebih umum, biasanya ekspresi yang dihasilkan tidak mudah
untuk diselesaikan secara analitik. Pada kasus ini biasanya digunakan
metode numerik seperti metode penurunan gradien (*gradient descent*).

Kita mulai dengan menuliskan fungsi rugi (@eq-loss-function-01) sebagai
$$
\begin{align*}
\mathcal{L} & = \frac{1}{N} \sum_{n=1}^{N} \left( t_n - (w_0 + w_1 x_{n}) \right)^2 \\
& = \frac{1}{N} \sum_{n=1}^{N} \left( w_1^2 x_n^2 + 2w_{1}x_{n}(w_0 - t_n) +
w_0^2 - 2w_0 t_n + t_n^2 \right)
\end{align*}
$$

Kemudian cari turunan dari $\mathcal{L}$ terhadap
$w_0$, $w_1$ dan menyamakan mereka dengan nol:
$$
\begin{align*}
\frac{\partial\mathcal{L}}{\partial w_1} & = 2w_1 \frac{1}{N} \left( \sum_{n=1}^{N} x_n^2 \right) +
\frac{2}{N} \left( \sum_{n=1}^{N} x_{n} (w_0 - t_n) \right) = 0 \\
\frac{\partial \mathcal{L}}{\partial w_0} & = 2w_0 + 2w_1 \frac{1}{N} \left( \sum_{n=1}^{N} x_n \right) -
\frac{2}{N} \left( \sum_{n=1}^{N} t_n \right) = 0
\end{align*}
$$

Dengan menyelesaikan persamaan ini diperoleh:
$$
\begin{align}
\begin{split}
w_{1} & = \frac{\overline{xy} - \overline{x}\overline{y}}{\overline{x^2} - \overline{x}^2} \\
w_{0} & = \overline{y} - w_{1} \overline{x}
\end{split}
\end{align}
$$ {#eq-w0-w1-analitik}
di mana notasi *overline* digunakan untuk menyatakan nilai rata-rata:
$$
\begin{align*}
\overline{x} & = \frac{1}{N} \sum_{n=1}^{N} x_{n} \\
\overline{y} & = \frac{1}{N} \sum_{n=1}^{N} y_{n}
\end{align*}
$$

Kode Python berikut ini mengimplementasikan perhitungan pada Persamaan (@eq-w0-w1-analitik).
Selain itu, kode ini juga menghitung hasil prediksi yang diperoleh dengan model linear
pada Persamaan (@eq-model-linear-01) dan membuat plot perbandingan dengan data.

```{python}
ybar = np.mean(y)
xbar = np.mean(x)
xybar = np.mean(x*y)
x2bar = np.mean(x**2)
  
w1 = (xybar - xbar*ybar)/(x2bar - xbar**2)
w0 = ybar - w1*xbar
  
print("Model parameters:")
print("w0 = %18.10e" % w0)
print("w1 = %18.10e" % w1)
  
y_pred = w0 + w1*x # garis lurus
  
plt.clf()
plt.plot(x, y, marker="o", linewidth=0, label="data")
plt.plot(x, y_pred, marker="x", label="linear-fit")
plt.grid(True)
plt.legend()
plt.xlabel("x")
plt.ylabel("y");
```

Bandingkan hasil dari $w_0$ dan $w_1$ yang sudah diperoleh dengan model yang sebenarnya.
Apakah mereka sama persis, cukup berdekatan, atau sangat jauh berbeda?

# Notasi vektor matriks

Pada bagian ini, kita akan menuliskan kembali permasalahan regresi linear
pada bagian sebelum dengan menggunakan notasi matriks dan vektor. Notasi ini
akan memungkinkan kita untuk menuliskan solusi $w_0$ dan $w_1$ menjadi lebih
kompak dan dapat diperumum untuk masalah regresi linear yang lebih luas.

Kita mulai dengan mendefinisikan vektor input dan vektor parameter atau
bobot sebagai berikut.
$$
\mathbf{x}_{n} = \begin{bmatrix}
1 \\
x_{n}
\end{bmatrix}
,\,\,\,%
\mathbf{w} = \begin{bmatrix}
w_{0} \\
w_{1}
\end{bmatrix}
$$

Dengan menggunakan definisi ini, model linear pada (@eq-model-linear-01)
menjadi
$$
f(x_n; w_0, w_1) = \mathbf{w}^{\mathsf{T}} \mathbf{x}_{n}
\label{}
$$ {#eq-model-linear-02}

Ekspresi untuk fungsi rugi, (@eq-loss-function-01),
menjadi:
$$
\mathcal{L} = \frac{1}{N} \sum_{n=1}^{N} \left( t_{n} - \mathbf{w}^{\mathsf{T}}
\mathbf{x}_{n} \right)^2
\label{eq:loss_function_02}
$$

Vektor input dapat disusun menjadi suatu matriks:
$$
\mathbf{X} = \begin{bmatrix}
\mathbf{x}^{\mathsf{T}}_{1} \\
\mathbf{x}^{\mathsf{T}}_{2} \\
\vdots \\
\mathbf{x}^{\mathsf{T}}_{N}
\end{bmatrix} =
\begin{bmatrix}
1 & x_{1} \\
1 & x_{2} \\
\vdots & \vdots \\
1 & x_{N} \\
\end{bmatrix}
$$

Kita juga dapat mendefinisikan vektor target sebagai
$$
\mathbf{y} = \begin{bmatrix}
y_1 \\
y_2 \\
\vdots \\
y_N
\end{bmatrix}
$$

Dengan definisi ini, kita dapat menuliskan fungsi rugi menjadi:
$$
\mathcal{L} = \frac{1}{N} \left( \mathbf{t} - \mathbf{Xw} \right)^{\mathsf{T}}
\left( \mathbf{t} - \mathbf{Xw} \right)
$$
Dengan mengalikan suku-suku dalam tanda kurung diperoleh:
$$
\begin{align}
\mathcal{L} & = \frac{1}{N} \left(
\mathbf{y}^{\mathsf{T}} \mathbf{y} +
\left(\mathbf{Xw}\right)^{\mathsf{T}} \mathbf{Xw} -
\mathbf{y}\mathbf{Xw} -
\left(\mathbf{Xw}\right)^{\mathsf{T}} \mathbf{y}
\right) \\
& = \frac{1}{N} \left(
\mathbf{w}^{\mathsf{T}} \mathbf{X}^{\mathsf{T}} \mathbf{X} \mathbf{w} -
2 \mathbf{w}^{\mathsf{T}} \mathbf{X}^{\mathsf{T}}\mathbf{y} +
\mathbf{y}^{\mathsf{T}} \mathbf{y}
\right)
\end{align}
$$

Turunan dari $\mathcal{L}$ terhadap $\mathbf{w}$ adalah
$$
\frac{\partial \mathcal{L}}{\partial \mathbf{w}} = \frac{2}{N}
\mathbf{X}^{\mathsf{T}} \mathbf{X} \mathbf{w} -
\frac{2}{N} \mathbf{X}^{\mathsf{T}} \mathbf{y} = \mathbf{0}
$$

Untuk menentukan nilai terbaik dari $\mathbf{w}$ kita dapat menggunakan
prosedur yang sama, yaitu dengan mencari solusi dari
$$
\dfrac{\partial \mathcal{L}}{\partial \mathbf{w}} = \mathbf{0}
$$
sehingga diperoleh
$$
\mathbf{X}^{\mathsf{T}} \mathbf{Xw} = \mathbf{X}^{\mathsf{T}}\mathbf{y} 
$$
Dengan menyelesaikan persamaan ini untuk $\mathbf{w}$ akhirnya diperoleh
$$
\mathbf{w} = \left(\mathbf{X}^{\mathsf{T}}\mathbf{X} \right)^{-1} \mathbf{X}^{\mathsf{T}} \mathbf{y}
$$ {#eq-w-vektor}

Kode Python berikut akan mengimplementasikan Persamaan (@eq-w-vektor) untuk regresi linear.

```{python}
Ndata = len(x)
# Build the input matrix or design matrix
X = np.zeros((Ndata,2))
X[:,0] = 1.0
X[:,1] = x
  
# Calculate the model parameters
XtX = X.T @ X   # @ adalah operator perkalian matriks pada Numpy
XtXinv = np.linalg.inv(XtX) # hitung inverse:
w = XtXinv @ X.T @ y
  
print("Model parameters:")
print("w0 = %18.10e" % w[0])
print("w1 = %18.10e" % w[1])
  
t_pred = X @ w
```

```{python}
X
```

```{python}
y
```

Konfirmasi bahwa solusi $w_0$ dan $w_1$ yang diperoleh sama dengan solusi dari
Persamaan (@eq-w0-w1-analitik).

Alternatif, menggunakan fungsi `np.linalg.lstsq`:

```{python}
res = np.linalg.lstsq(X, y, rcond=-1)
```

```{python}
res
```

```{python}
w, *_ = np.linalg.lstsq(X, y, rcond=-1)
```

```{python}
w
```

Coba bandingkan hasil yang diperoleh dari `np.linalg.lstsq`
dengan hasil dari Persamaan (@eq-w0-w1-analitik). 

# Penggunaan Scikit Learn

Kelas `LinearRegression`:

```python
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(x, y)

# Evaluate model performance
R2_score = model.score(x, y)
print("R2_score on training data: ", R2_score)

# Prediction on training data
y_pred = model.predict(x)
RMSE = np.sqrt(np.mean((y_pred - y)**2))
print("RMSE training: ", RMSE)
```

```{python}
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X, y);
```

```{python}
model.intercept_   # w0
```

```{python}
model.coef_  # w1
```

```{python}
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(x[:,np.newaxis], y);
```

```{python}
model.coef_ # w1
```

```{python}
model.intercept_ # w0
```
